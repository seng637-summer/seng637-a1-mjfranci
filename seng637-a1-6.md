>   **SENG 637 - Dependability and Reliability of Software Systems**

**Lab. Report \#1 – Introduction to Testing and Defect Tracking**

| Group: 6      |
|-----------------|
| Francis, Michael                |   
| Le, Michael              |   
| Pretorius, Jean-Charl               |   
| Rainbow, Sam                |
| Sofela, Samuel                |   


**Table of Contents**

[1 Introduction	1](#_Toc439194677)

[2 High-level description of the exploratory testing plan	1](#_Toc439194678)

[3 Comparison of exploratory and manual functional testing	1](#_Toc439194679)

[4 Notes and discussion of the peer reviews of defect reports	1](#_Toc439194680)

[5 How the pair testing was managed and team work/effort was
divided	1](#_Toc439194681)

[6 Difficulties encountered, challenges overcome, and lessons
learned	1](#_Toc439194682)

# Introduction

This assignment was designed to help us develop an understanding of the software testing process. We familiarize ourselves with the System Under Test (SUT) – an ATM Simulation application – and Atlassian Jira, a code defect tracking application. We practiced exploratory testing, manual scripted testing, and regression testing. 

Before the start of this assignment, our group possessed a rudimentary understanding of exploratory and manual functional testing from the first week of SENG 637 lectures. Exploratory testing looks for code defects by varying inputs, states, environments, user data, and observing the code output. Manual scripted testing looks for code defects by following a set of written test cases and instructions that are documented by the tester. Manual regression testing is performed to ensure that a system update does not re-introduce faults that were corrected from an earlier version.

While we possessed this understanding, none of us had any previous experience with performing these tests on a SUT or using an issue tracking system to create defect reports.


# High-level description of the exploratory testing plan

The following is a high-level description of our exploratory testing plan:

1. First, we examined the requirements of the ATM simulation System Under Test that were outlined in Appendix B of the assignment description on GitHub.
2. From these requirements, we derive a list of main functions to target for testing:
    - Card Validation and PIN Entry
    - Cash Withdrawal
    - Deposit (Cash and Checks)
    - Money Transfer
    - Balance Inquiry
    - Transaction Abort
    - Receipt Generation
    - Operator Controls
    - Transaction Logging
    - Card Retention and Return
    - Number of Bills
3. For each function listed above, we consider different expected and unexpected inputs and actions, boundary conditions, error handling, risk prioritization, and usability and user experience. 
4. Finally, we perform exploratory testing by testing each scenario and recording the initial system state, the steps taken to produce a defect or bug, the expected result, actual result, and whether or not the test passed or failed. Throughout this process, the members of each pair in our group alternated between testing the application and recording the results in the table. Each possible scenario is listed in a table that is filled out as exploratory testing is done (this differs from manual functional testing which has test cases written out before testing). An example of our exploratory testing for operator controls function is presented in Table 1.
5. Once exploratory testing is completed, the defects are recorded in the Jira issue tracking system.

**Table 1.** Exploratory Unit Tests for Operator Control Function
|**Function Tested**|**Initial State of System**|**Steps Taken to Produce Bug**|**Expected Outcome**|**Actual Outcome**| **Pass/Fail**|
|---|---|---|---|---|---|
|Operator Controls - Switching ON|Not servicing customer; Message: “Not Currently Available”; Message: “Click Button to turn ATM ON”|Click ON button|Machine turns ON and asks operator to enter the number of $20.00 bills|Machine turns ON and asks operator to enter the number of $20.00 bills|Pass|
|Operator Controls - Switching OFF|Not servicing customer; Message: “Not Currently Available”; Message: “Click Button to turn ATM ON”|Click OFF button|Machine turns OFF|Machine turns OFF|Pass|
|Operator Controls - Switching OFF|Servicing customer Card #: 1|Click OFF button|Machine does not turn OFF|Machine does not turn OFF|Pass|

# Comparison of exploratory and manual functional testing

There is a significant difference between exploratory testing and manual functional testing. Exploratory testing is an approach where the test design, execution, and evaluation are performed simultaneously. Unit tests are defined as testing occurs, instead of a priorly built test suite like in manual functional testing. In our assignment, we performed exploratory testing on the features and functionalities of the ATM simulation application without relying heavily on predefined test cases. Throughout exploratory testing, we interact with the ATM simulation system, observe its behavior, and identify defects, usability issues, and potential risks. Compared to manual functional testing, exploratory testing is more flexible and adaptable, allowing testers to follow their intuition and adapt their testing based on real-time feedback. The trade-off is that exploratory testing relies on the skills and experience of the tester, and runs the risk of being poorly documented.

Manual functional testing is a structured approach where testers follow predefined test cases and test scripts to verify the functionality of an application. Unlike exploratory testing, test cases are designed in advance. Specific inputs, expected outputs, and steps to be followed are outlined. Testers execute the tests step-by-step, record the results, and compare them against expected outcomes. Manual functional testing is more systematic and predictable, providing a clear path for test coverage and repeatability. The downside of this approach is that the scope of the tests will be limited to the predefined instructions, and great care needs to be given to form a quality test suite.

# Notes and discussion of the peer reviews of defect reports

It was interesting to see the variety in approaches that team members had when executing the tests and how that was reflected by each group identifying some different bugs. It is clear that a peer review process is important for a high-quality defect report. Having multiple people working on the same project is always a good idea. One person doing all the testing means that there is a risk that a defect is missed. This was evident when we were conducting the exploratory tests, as the observer would catch bugs that the tester would miss.

It is also clear that a common logging system is needed. Having different formats and approaches creates extra work for the entire team. Miscommunication can lead to defects not being logged, and thus not being fixed. The team needs to be on the same page about how they will communicate with each other.

# How the pair testing was managed and team work/effort was divided 

The group was divided into two subgroups of two and three members to conduct “pair testing” on the software system. Approximately 30 minutes to an hour of testing was performed while following the high-level exploratory test plan. The group members not at the keyboard – the “observers” - recorded any bugs encountered as well as the conditions surrounding the bug. Doing the exploratory testing with one or two other people was highly effective at identifying more defects. The observers would share their perspective on the test by giving suggestions to the tester to help them catch details they may have missed. After the two groups were finished with exploratory testing the defects were reviewed and entered in to separate Jira projects. 

Manual scripted testing was performed in a very similar way, with one group member testing the system and the others observing which tests have been completed and writing down any defects found. The tests were executed in the same order as they are numbered in the table of test cases, and both pairs completed all 40 test cases. After completing the MFT testing, the test cases were entered into Jira and the group member who was driving the testing reviewed the defect reports. 

Regression testing was performed by the two groups with all of the defects divided among the group members and retested on version 1.1.  Defects that were fixed in version 1.1 are now labeled as “DONE” and those not yet fixed are labeled “IN-PROGRESS”.

Finally, the defect reports from the two groups were reviewed and combined into a single Jira project.

# Difficulties encountered, challenges overcome, and lessons learned

The group learned the importance of creating and following a testing plan. We learned that it ensures robust testing is performed and that team members understand the process and work that needs to be done. Devising an appropriate plan for the exploratory testing was a challenge because the group did not have experience making a software testing plan that would cover the necessary scenarios and ensure that the basic use cases were covered. 

The group also gained practical experience with exploratory testing, manual scripted testing, and regression testing, as well as how to work together in a “pair testing” environment. Identifying exactly what does and does not characterize a “defect” and how the context of the test being performed can inform the decision of whether the test passes was sometimes challenging, and it sparked great discussion between group members.
Learning how to use an issue tracking system such as Jira was not too difficult and has proved to be a valuable outcome of this assignment. 

Looking back there was a lot of duplicate work done by the group by having two sets of bugs tracked in two separate Jira projects and it would have been more efficient for the two groups to write the details of the bugs down and then combine the issues into a single Jira project at an earlier stage in the assignment. Combining these defects and eliminating duplicates was a challenge. A lesson learned is to better plan and communicate the work that needs to be done before the two pairs break off and work on the assignment.